{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67936f88-5e54-4d3c-92b6-76960c7a3c68",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Merging Green Taxi dataset with Yellow Taxi dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2805780-ee90-4f22-a913-507236587bb5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Preparation of Green Taxi dataset before merge: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a71d5ea-5a54-40d8-a925-b4910c8854f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[2]: 62758794"
     ]
    }
   ],
   "source": [
    "clean_green = spark.read.parquet(\"/CleanedData/green/cleaned_*\")\n",
    "clean_green.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6ccfdbc-2595-4234-87de-0a1c2bdb89b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- VendorID: long (nullable = true)\n |-- tpep_pickup_datetime: timestamp (nullable = true)\n |-- tpep_dropoff_datetime: timestamp (nullable = true)\n |-- store_and_fwd_flag: string (nullable = true)\n |-- RatecodeID: double (nullable = true)\n |-- PULocationID: long (nullable = true)\n |-- DOLocationID: long (nullable = true)\n |-- passenger_count: double (nullable = true)\n |-- trip_distance: double (nullable = true)\n |-- fare_amount: double (nullable = true)\n |-- extra: double (nullable = true)\n |-- mta_tax: double (nullable = true)\n |-- tip_amount: double (nullable = true)\n |-- tolls_amount: double (nullable = true)\n |-- ehail_fee: double (nullable = true)\n |-- improvement_surcharge: double (nullable = true)\n |-- total_amount: double (nullable = true)\n |-- payment_type: double (nullable = true)\n |-- trip_type: double (nullable = true)\n |-- congestion_surcharge: double (nullable = true)\n |-- filename: string (nullable = true)\n |-- trip_speed: double (nullable = true)\n |-- trip_duration: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "clean_green.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "307dd1cb-51d5-4e4a-b13f-fbca25c3a25b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "No information about ehail_fee has been given in the data dictionary. It is therefore being dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e96876c9-7bff-49d3-b99e-6d4787798486",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n0.0\n1.95\n"
     ]
    }
   ],
   "source": [
    "unique_ehail_fee_values = clean_green.select(\"ehail_fee\").distinct().collect()\n",
    "\n",
    "# Print the unique values\n",
    "for row in unique_ehail_fee_values:\n",
    "    print(row.ehail_fee)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63dc7b5f-d92a-4df8-b510-77dd91cb2efc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Schemas of Green Taxi dataset and Yellow Taxi dataset not only need to be same but the columns from both the dataset need to be in the exact order for the merge to be successful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00926f5a-ab60-4845-b89d-f7afccd0123c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "\n",
    "output_folder = \"/CombinedCleanedData/\"\n",
    "\n",
    "output_filename = output_folder + \"cleaned_green_transformed.parquet\"\n",
    "\n",
    "\n",
    "# Drop the 'ehail_fee' column from the DataFrame\n",
    "clean_green = clean_green.drop(\"ehail_fee\")\n",
    "clean_green = clean_green.withColumn(\"color\", lit(\"green\")) #Adding the color column so that the Green Taxi dataset can be processed separately later for future queries. \n",
    "clean_green = clean_green.withColumn(\"airport_fee\", lit(None).cast(DoubleType())) # Matching the datatypes and comparing with the schema of Yellow taxi dataset below. \n",
    "clean_green = clean_green.withColumn(\"payment_type\", col(\"payment_type\").cast(\"long\")) # Matching the datatypes and comparing with the schema of Yellow taxi dataset below. \n",
    "\n",
    "clean_green.write.parquet(output_filename, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c58520cf-160f-47a7-bd60-8cffb8502fdc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Tranforming both the datasets so their schemas match. Both the dataframes are going to be saved in the CombinedCleanedData folder as shown below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ee2643d-2303-4f61-a494-495cf74951a1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Comparing the schema with that of Yellow Taxi dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48ce9a02-2382-4510-b81a-ab9d885359a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[6]: 646503433"
     ]
    }
   ],
   "source": [
    "clean_yellow = spark.read.parquet(\"/CleanedData/yellow/cleaned_*\")\n",
    "clean_yellow.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b5a8275-32c6-4e13-9514-bd93be333501",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- VendorID: long (nullable = true)\n |-- tpep_pickup_datetime: timestamp (nullable = true)\n |-- tpep_dropoff_datetime: timestamp (nullable = true)\n |-- passenger_count: double (nullable = true)\n |-- trip_distance: double (nullable = true)\n |-- RatecodeID: double (nullable = true)\n |-- store_and_fwd_flag: string (nullable = true)\n |-- PULocationID: long (nullable = true)\n |-- DOLocationID: long (nullable = true)\n |-- payment_type: long (nullable = true)\n |-- fare_amount: double (nullable = true)\n |-- extra: double (nullable = true)\n |-- mta_tax: double (nullable = true)\n |-- tip_amount: double (nullable = true)\n |-- tolls_amount: double (nullable = true)\n |-- improvement_surcharge: double (nullable = true)\n |-- total_amount: double (nullable = true)\n |-- congestion_surcharge: double (nullable = true)\n |-- airport_fee: double (nullable = true)\n |-- filename: string (nullable = true)\n |-- trip_speed: double (nullable = true)\n |-- trip_duration: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "clean_yellow.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9c0f418-7386-4ce5-8d91-bca6a5e37f45",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "\n",
    "clean_yellow = clean_yellow.withColumn(\"color\", lit(\"yellow\"))\n",
    "clean_yellow = clean_yellow.withColumn(\"trip_type\",lit(None).cast(DoubleType()))\n",
    "output_folder = \"/CombinedCleanedData/\"\n",
    "\n",
    "output_filename = output_folder + \"cleaned_yellow_transformed.parquet\"\n",
    "\n",
    "clean_yellow.write.parquet(output_filename, mode=\"overwrite\") #Saving the Tranformed Yellow Dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "493a867f-8948-4c5f-917e-ed26c455c892",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- VendorID: long (nullable = true)\n |-- tpep_pickup_datetime: timestamp (nullable = true)\n |-- tpep_dropoff_datetime: timestamp (nullable = true)\n |-- passenger_count: double (nullable = true)\n |-- trip_distance: double (nullable = true)\n |-- RatecodeID: double (nullable = true)\n |-- store_and_fwd_flag: string (nullable = true)\n |-- PULocationID: long (nullable = true)\n |-- DOLocationID: long (nullable = true)\n |-- payment_type: double (nullable = true)\n |-- fare_amount: double (nullable = true)\n |-- extra: double (nullable = true)\n |-- mta_tax: double (nullable = true)\n |-- tip_amount: double (nullable = true)\n |-- tolls_amount: double (nullable = true)\n |-- improvement_surcharge: double (nullable = true)\n |-- total_amount: double (nullable = true)\n |-- congestion_surcharge: double (nullable = true)\n |-- airport_fee: double (nullable = true)\n |-- filename: string (nullable = true)\n |-- trip_speed: double (nullable = true)\n |-- trip_duration: long (nullable = true)\n |-- color: string (nullable = true)\n |-- trip_type: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "clean_green.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ec236d1-36b5-4e86-9c20-2103df7509f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[5]: 646503433"
     ]
    }
   ],
   "source": [
    "clean_yellow = spark.read.parquet(\"/CombinedCleanedData/cleaned_yellow_transformed.parquet\")\n",
    "clean_yellow.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77e25430-a83a-4f9d-8a07-66e211975ac9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "output_folder = \"/CombinedCleanedData/\"\n",
    "\n",
    "output_filename = output_folder + \"cleaned_green_transformed.parquet\"\n",
    "\n",
    "\n",
    "# Define the desired column order based on your provided schema\n",
    "desired_column_order = [\n",
    "    \"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \n",
    "    \"passenger_count\", \"trip_distance\", \"RatecodeID\", \"store_and_fwd_flag\", \n",
    "    \"PULocationID\", \"DOLocationID\", \"payment_type\", \"fare_amount\", \n",
    "    \"extra\", \"mta_tax\", \"tip_amount\", \"tolls_amount\", \"improvement_surcharge\", \n",
    "    \"total_amount\", \"congestion_surcharge\", \"airport_fee\", \"filename\", \n",
    "    \"trip_speed\", \"trip_duration\", \"color\", \"trip_type\"\n",
    "]\n",
    "\n",
    "# Select columns in the desired order\n",
    "clean_green = clean_green.select(*desired_column_order)\n",
    "clean_green.write.parquet(output_filename, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f815aa4f-4048-4313-84a2-d7081b3e2b6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "clean_green = clean_green.withColumn(\"payment_type\", col(\"payment_type\").cast(\"long\"))\n",
    "clean_green.write.parquet(output_filename, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baa64e7c-e306-4651-8e8c-580311c633f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[2]: 62758794"
     ]
    }
   ],
   "source": [
    "clean_green = spark.read.parquet(\"/CombinedCleanedData/cleaned_green_transformed.parquet\")\n",
    "clean_green.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "400eb474-7710-4886-8717-24a8679e1ff0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Combining both the transformed dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88899477-03bf-4a3b-ab29-3e65a404708a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "clean_green = spark.read.parquet(\"/CombinedCleanedData/cleaned_green_transformed.parquet\")\n",
    "clean_yellow = spark.read.parquet(\"/CombinedCleanedData/cleaned_yellow_transformed.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31841e2d-c8d9-4a02-aa43-0844ff1c10dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b928ee72-5742-4380-90e5-7330f71a119d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Few columns were added during the data cleaning phase for filtering a few erroneous entries. Those columns will be removed to make the merge process smoother. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aeb963c-1965-4675-83ba-5b18cef59187",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List of columns to remove\n",
    "columns_to_remove = [\"filename\", \"trip_speed\", \"trip_duration\", \"trip_type\"]\n",
    "\n",
    "# Select only the columns you want to keep\n",
    "clean_yellow = clean_yellow.select([col for col in clean_yellow.columns if col not in columns_to_remove])\n",
    "output_folder = \"/CombinedCleanedData/\"\n",
    "\n",
    "output_filename = output_folder + \"cleaned_yellow_newcols.parquet\"\n",
    "\n",
    "clean_yellow.write.parquet(output_filename, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df702e68-59a9-4bdf-93dc-3243340af171",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Unfortunately it took too long to apply both tranformations (removal of columns) and merge at the same time. The transformed Green and Yellow steps are saved as intermediary parquet files to facilate the merge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "752a2cb5-ec76-4583-9906-9ab11f4398f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "clean_green = spark.read.parquet(\"/CombinedCleanedData/cleaned_green_transformed.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d2192d0-1f29-4604-b01d-2808e3ab9cea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List of columns to remove\n",
    "columns_to_remove = [\"filename\", \"trip_speed\", \"trip_duration\", \"trip_type\"]\n",
    "\n",
    "# Select only the columns you want to keep\n",
    "clean_green = clean_green.select([col for col in clean_green.columns if col not in columns_to_remove])\n",
    "output_folder = \"/CombinedCleanedData/\"\n",
    "\n",
    "output_filename = output_folder + \"cleaned_green_newcols.parquet\"\n",
    "\n",
    "clean_green.write.parquet(output_filename, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6affef3a-aaf8-485a-8087-98889fa62e94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The schemas are equal.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if clean_yellow.schema == clean_green.schema:\n",
    "    print(\"The schemas are equal.\")\n",
    "else:\n",
    "    print(\"The schemas are not equal.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1834c600-104d-4ea6-b22c-023c48b75680",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "clean_green = spark.read.parquet(\"/CombinedCleanedData/cleaned_green_newcols.parquet\")\n",
    "clean_yellow = spark.read.parquet(\"/CombinedCleanedData/cleaned_yellow_newcols.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9498d681-45c9-4f17-a985-020f0262c4a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[5]: DataFrame[VendorID: bigint, tpep_pickup_datetime: timestamp, tpep_dropoff_datetime: timestamp, passenger_count: double, trip_distance: double, RatecodeID: double, store_and_fwd_flag: string, PULocationID: bigint, DOLocationID: bigint, payment_type: bigint, fare_amount: double, extra: double, mta_tax: double, tip_amount: double, tolls_amount: double, improvement_surcharge: double, total_amount: double, congestion_surcharge: double, airport_fee: double, filename: string, trip_speed: double, trip_duration: bigint, color: string]"
     ]
    }
   ],
   "source": [
    "clean_green.cache()\n",
    "clean_yellow.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8421da28-f600-40a4-85b3-12c206f61ebd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[6]: 709262227"
     ]
    }
   ],
   "source": [
    "combined_df = clean_yellow.union(clean_green)\n",
    "combined_df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc55c4e7-8b29-458c-978f-16acded37ecd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "combined_df.write.mode('append').parquet('/CombinedCleanedData/cleaned_combined.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0eb31dce-15dc-450b-985f-431bffd0abf7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Following the above steps, we were able to successfully merge both the Green and the Yellow taxi datasets. "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Merge Dataset-Part-2",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
