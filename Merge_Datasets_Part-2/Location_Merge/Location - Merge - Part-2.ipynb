{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "982b859d-19d7-4e5c-862b-7945662fe82f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Unioning the cleaned combined data with Location data\n",
    "\n",
    "In this data processing project, our objective was to enhance a vast taxi trip dataset by enriching it with location information from a separate lookup dataset. The goal was to perform joins based on pickup and dropoff locations, ultimately providing valuable insights for further analysis. However, the journey to achieving this task revealed a set of challenges that led to an incomplete join between the datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "803d6032-a48f-4514-a34a-252b6b39ca5b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Here are the steps to combine the new dataframe with the location data (pick up and drop off locations) and export it to a Parquet file in Databricks DBFS, and then load it as a table or view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67b72886-2c3a-416c-9120-3346ac3f1f32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[1]: 709262227"
     ]
    }
   ],
   "source": [
    "combined_data = spark.read.parquet(\"/CombinedCleanedData/cleaned_combined.parquet\")\n",
    "combined_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f59cb4e4-a9d4-494f-86f7-28afc66bc00b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- VendorID: long (nullable = true)\n |-- tpep_pickup_datetime: timestamp (nullable = true)\n |-- tpep_dropoff_datetime: timestamp (nullable = true)\n |-- passenger_count: double (nullable = true)\n |-- trip_distance: double (nullable = true)\n |-- RatecodeID: double (nullable = true)\n |-- store_and_fwd_flag: string (nullable = true)\n |-- PULocationID: long (nullable = true)\n |-- DOLocationID: long (nullable = true)\n |-- payment_type: long (nullable = true)\n |-- fare_amount: double (nullable = true)\n |-- extra: double (nullable = true)\n |-- mta_tax: double (nullable = true)\n |-- tip_amount: double (nullable = true)\n |-- tolls_amount: double (nullable = true)\n |-- improvement_surcharge: double (nullable = true)\n |-- total_amount: double (nullable = true)\n |-- congestion_surcharge: double (nullable = true)\n |-- airport_fee: double (nullable = true)\n |-- color: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "combined_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bebb6d0-e6bb-4a9f-bf0c-f1bbead131a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lookup_df = spark.read.csv(\"dbfs:/FileStore/taxi_zone_lookup.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eda3d24b-b60d-4e5f-9b1d-8174f1397b28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- LocationID: integer (nullable = true)\n |-- Borough: string (nullable = true)\n |-- Zone: string (nullable = true)\n |-- service_zone: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "lookup_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a67b84b-bd45-4719-a840-b72b4dd93272",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[2]: True"
     ]
    }
   ],
   "source": [
    "#dbutils.fs.rm(\"/MergedData/2016_parquet\", True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47c2dd09-29ce-48ae-926c-0a74be06a008",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"Merge and Save\").getOrCreate()\n",
    "\n",
    "# Read the combined data\n",
    "combined_data = spark.read.parquet(\"/CombinedCleanedData/cleaned_combined.parquet\")\n",
    "\n",
    "# Read the lookup data\n",
    "lookup_df = spark.read.csv(\"dbfs:/FileStore/taxi_zone_lookup.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Select the relevant columns from the lookup data and rename them\n",
    "lookup_df = lookup_df.selectExpr(\"LocationID as PULocationID\", \"Borough as PULocation_Borough\", \"Zone as PULocation_Zone\")\n",
    "\n",
    "# Rename the PULocationID column in the combined_data\n",
    "combined_data = combined_data.withColumnRenamed(\"PULocationID\", \"Combined_PULocationID\")\n",
    "\n",
    "# Join the combined_data with the lookup_df on PULocationID\n",
    "merged_data = combined_data.join(lookup_df, combined_data[\"Combined_PULocationID\"] == lookup_df[\"PULocationID\"], \"left\")\n",
    "\n",
    "# Save the merged data as a Parquet file with a specific name\n",
    "merged_data.write.parquet(\"/MergedData/merged_pulocation.parquet\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7508f2c-42d6-4873-846d-1c37cea9422e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[3]: 605365170"
     ]
    }
   ],
   "source": [
    "merged_data= spark.read.parquet(\"/MergedData/merged_pulocation.parquet/*\")\n",
    "merged_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4494740b-5b98-447f-9280-cc20f50aae82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "The spark context has stopped and the driver is restarting. Your notebook will be automatically reattached.",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"Merge and Save\").getOrCreate()\n",
    "\n",
    "# Read the combined data\n",
    "combined_data = spark.read.parquet(\"/MergedData/merged_pulocation.parquet/*\")\n",
    "\n",
    "# Read the lookup data\n",
    "lookup_df = spark.read.csv(\"dbfs:/FileStore/taxi_zone_lookup.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Select the relevant columns from the lookup data and rename them\n",
    "lookup_df = lookup_df.selectExpr(\"LocationID as DOLocationID\", \"Borough as DOLocation_Borough\")\n",
    "\n",
    "# Rename the DOLocationID column in the combined_data\n",
    "combined_data = combined_data.withColumnRenamed(\"DOLocationID\", \"Combined_DOLocationID\")\n",
    "\n",
    "# Join the combined_data with the lookup_df on DOLocationID\n",
    "merged_data = combined_data.join(lookup_df, combined_data[\"Combined_DOLocationID\"] == lookup_df[\"DOLocationID\"], \"left\")\n",
    "\n",
    "# Save the merged data as a Parquet file with a specific name\n",
    "merged_data.write.parquet(\"/MergedData/merged_both_locations.parquet\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "feff189c-964f-46a4-a5dc-6992ccf78127",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[1]: 605365170"
     ]
    }
   ],
   "source": [
    "merged_data = spark.read.parquet(\"/MergedData/merged_both_locations.parquet\")\n",
    "merged_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fbcb15b-959c-4366-855e-67c2a248e96b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Some Highlights:\n",
    "\n",
    "\n",
    "In our data project, we aimed to combine a massive taxi trip dataset with location information. However, we faced three big challenges:\n",
    "\n",
    "1. **Data Size:** Our combined dataset had a staggering 709,262,227 entries. Handling such a huge dataset was tough.\n",
    "\n",
    "2. **Limited Resources:** We didn't have unlimited computing power. Our computers struggled to process the large dataset, making the work slow.\n",
    "\n",
    "3. **Data Growth:** The dataset kept growing, which made things even harder. It was like trying to solve a puzzle that kept getting bigger.\n",
    "\n",
    "In simple terms, our problem wasn't the quality of the location dataâ€”it was the sheer size of our taxi dataset. This project taught us that working with big data is tough, even with the best tools.\n",
    "\n",
    "In the end, we couldn't completely combine the location data. But we learned a lot about managing data, planning, and making the most of what we have. It's been an interesting journey, reminding us that big data is both challenging and full of exciting possibilities.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Location - Merge - Part-2",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
