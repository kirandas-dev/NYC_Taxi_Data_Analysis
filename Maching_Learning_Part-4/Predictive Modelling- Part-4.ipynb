{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbca88ba-7230-47ab-81c9-31088f3157d1",
     "showTitle": false,
     "title": ""
    },
    "id": "w0CDQfUlvaTl"
   },
   "source": [
    "# **Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e96552a-158e-4354-9167-58536b52d932",
     "showTitle": false,
     "title": ""
    },
    "id": "qIbjP65vvaTn"
   },
   "source": [
    "# Taxi Fare Prediction using Spark ML\n",
    "\n",
    "In this notebook, we will explore the task of predicting taxi fare amounts based on various features related to taxi trips in New York City. We will utilize the power of Apache Spark and its Machine Learning library, Spark ML, to build and evaluate predictive models.\n",
    "\n",
    "## Dataset Overview\n",
    "\n",
    "The dataset we are working with contains a wealth of information about taxi trips, including pick-up and drop-off times, passenger counts, trip distances, fare amounts, and more. Here's a brief overview of the dataset:\n",
    "\n",
    "- **VendorID:** A code indicating the TPEP provider.\n",
    "- **tpep_pickup_datetime:** The time when the trip started.\n",
    "- **tpep_dropoff_datetime:** The time when the trip ended.\n",
    "- **Passenger_count:** The number of passengers in the taxi.\n",
    "- **Trip_distance:** The distance of the trip in miles.\n",
    "- **RateCodeID:** The rate code for the trip (e.g., standard rate, airport fare).\n",
    "- **Other Features:** There are additional features such as `Hour_of_Pickup`, `Day_of_Week`, `trip_duration`, and more, which we will engineer for our predictive models.\n",
    "\n",
    "## Objective\n",
    "\n",
    "Our primary goal is to build machine learning models that can accurately predict the total fare amount for taxi trips. We will explore different algorithms and hyperparameter tuning techniques to achieve the best predictive performance.\n",
    "\n",
    "## Approach\n",
    "\n",
    "1. **Data Preprocessing:** We will start by loading and preprocessing the dataset, including handling missing values, encoding categorical features, and creating new features.\n",
    "   \n",
    "2. **Feature Selection:** We will analyze the dataset to select the most relevant features for our prediction task.\n",
    "   \n",
    "3. **Model Building:** We will experiment with multiple machine learning algorithms, including Random Forest, Gradient Boosting, and more.\n",
    "   \n",
    "4. **Hyperparameter Tuning:** We will fine-tune the hyperparameters of our models using techniques like grid search or random search.\n",
    "   \n",
    "5. **Model Evaluation:** We will evaluate the models using appropriate evaluation metrics such as Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and R-squared (R2).\n",
    "\n",
    "6. **Prediction:** Finally, we will use the best-performing model to make predictions on a test dataset and assess its performance.\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "Make sure you have set up your Spark environment with the necessary configurations and libraries. You can install required libraries using `pip` or `conda`.\n",
    "\n",
    "Let's get started with data loading and preprocessing!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26d3fdac-9abb-47c4-bf40-d53ad9cdcd19",
     "showTitle": false,
     "title": ""
    },
    "id": "fR2R5x5xGNTe"
   },
   "source": [
    "# 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "442b5ebf-9b32-4b98-93b6-9d52389ad44c",
     "showTitle": false,
     "title": ""
    },
    "id": "zd-xamrXGNTe"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "37411142"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data = spark.read.parquet(\"/finalds/combined/1.parquet\")\n",
    "combined_data.count()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e1e6483-cbe6-4ac3-b5fd-2619c72e9330",
     "showTitle": false,
     "title": ""
    },
    "id": "DmVaVqlSqLuD"
   },
   "source": [
    "Cache the dataframe `combined_data` using `.cache()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a553aee-d3e1-4eea-996c-366856238519",
     "showTitle": false,
     "title": ""
    },
    "id": "feK76OzhqLuD"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[VendorID: bigint, lpep_pickup_datetime: timestamp, lpep_dropoff_datetime: timestamp, store_and_fwd_flag: string, RatecodeID: double, PULocationID: bigint, DOLocationID: bigint, passenger_count: double, trip_distance: double, fare_amount: double, extra: double, mta_tax: double, tip_amount: double, tolls_amount: double, ehail_fee: double, improvement_surcharge: double, total_amount: double, payment_type: double, trip_type: double, congestion_surcharge: double]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1e7a909-196f-4da9-a6ae-4672cc9402c1",
     "showTitle": false,
     "title": ""
    },
    "id": "sSlUN-aNo2oG"
   },
   "source": [
    "Print the schema of `combined_data` using printSchema():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed00956f-e9c9-415d-9c65-45b4c442a2f6",
     "showTitle": false,
     "title": ""
    },
    "id": "TaxHWy5mpNhm"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- VendorID: long (nullable = true)\n |-- lpep_pickup_datetime: timestamp (nullable = true)\n |-- lpep_dropoff_datetime: timestamp (nullable = true)\n |-- store_and_fwd_flag: string (nullable = true)\n |-- RatecodeID: double (nullable = true)\n |-- PULocationID: long (nullable = true)\n |-- DOLocationID: long (nullable = true)\n |-- passenger_count: double (nullable = true)\n |-- trip_distance: double (nullable = true)\n |-- fare_amount: double (nullable = true)\n |-- extra: double (nullable = true)\n |-- mta_tax: double (nullable = true)\n |-- tip_amount: double (nullable = true)\n |-- tolls_amount: double (nullable = true)\n |-- ehail_fee: double (nullable = true)\n |-- improvement_surcharge: double (nullable = true)\n |-- total_amount: double (nullable = true)\n |-- payment_type: double (nullable = true)\n |-- trip_type: double (nullable = true)\n |-- congestion_surcharge: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "combined_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "013e3dad-8b9c-4758-8895-3942d046efe0",
     "showTitle": false,
     "title": ""
    },
    "id": "oFmbK2UNvaTw"
   },
   "source": [
    "Print the number of rows and columns of `combined_data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be2c8daa-834e-43fe-8fbb-357ca0842bd1",
     "showTitle": false,
     "title": ""
    },
    "id": "2Kt-2dwZvaTz"
   },
   "source": [
    "# 2. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a5960db-0551-41c4-b1de-8b92a9b15e0a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data Preprocessing: Filtering and Feature Engineering\n",
    "\n",
    "In this section, we perform essential data preprocessing tasks on our taxi trip dataset. These tasks include filtering the data for the years 2021 and 2022 and creating new features that will be used for our predictive modeling. Let's break down what we're doing:\n",
    "\n",
    "1. **Filtering Data for 2021 and 2022:** We start by filtering our dataset to include only taxi trips that occurred in the years 2021 and 2022. This step helps us focus on recent data for our analysis and model training.\n",
    "\n",
    "2. **Feature Engineering:** We engineer several features that are relevant for our prediction task:\n",
    "   - `Hour_of_Pickup`: We extract the hour component from the `tpep_pickup_datetime` column to understand the time of day when the trip started.\n",
    "   - `Day_of_Week`: We extract the day of the week from the `tpep_pickup_datetime` column to consider the day of the week's influence on fares.\n",
    "   - `trip_duration`: We calculate the trip duration in hours by computing the time difference between `tpep_pickup_datetime` and `tpep_dropoff_datetime` and converting it from seconds to hours.\n",
    "   - `Passenger_count`: We retain the passenger count as a feature.\n",
    "   - `trip_distance`: We keep the trip distance as a feature.\n",
    "   - `RateCodeID`: We retain the rate code, which represents the fare structure, as a feature.\n",
    "   - `total_amount`: We keep the total fare amount as our target variable for prediction.\n",
    "   - `tpep_pickup_datetime` and `tpep_dropoff_datetime`: We retain these columns for reference.\n",
    "\n",
    "3. **Data Saving:** After preprocessing and feature engineering, we save the filtered and transformed data as a new Parquet file for further analysis and model building.\n",
    "\n",
    "Now, let's proceed with the code to perform these preprocessing steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb11f6bb-1adc-4685-bdfa-99047e62763b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+-------------------+---------------+-------------+----------+------------+--------------------+---------------------+\n|Hour_of_Pickup|Day_of_Week|      trip_duration|Passenger_count|trip_distance|RateCodeID|total_amount|tpep_pickup_datetime|tpep_dropoff_datetime|\n+--------------+-----------+-------------------+---------------+-------------+----------+------------+--------------------+---------------------+\n|            23|          7| 0.3338888888888889|            1.0|          4.7|       1.0|       26.16| 2022-11-26 23:55:08|  2022-11-27 00:15:10|\n|            23|          7|0.17472222222222222|            1.0|         6.19|       1.0|        19.3| 2022-11-26 23:00:44|  2022-11-26 23:11:13|\n|            23|          7|               0.11|            2.0|          1.2|       1.0|       12.85| 2022-11-26 23:19:56|  2022-11-26 23:26:32|\n|            23|          7|             0.0725|            1.0|          0.6|       1.0|         8.3| 2022-11-26 23:34:11|  2022-11-26 23:38:32|\n|            23|          7| 0.1922222222222222|            2.0|          2.2|       1.0|        13.8| 2022-11-26 23:47:18|  2022-11-26 23:58:50|\n|            23|          7|0.19722222222222222|            5.0|         3.79|       1.0|        16.3| 2022-11-26 23:08:12|  2022-11-26 23:20:02|\n|            23|          7| 0.4311111111111111|            6.0|         2.43|       1.0|        26.0| 2022-11-26 23:30:46|  2022-11-26 23:56:38|\n|            23|          7|0.23666666666666666|            1.0|         3.33|       1.0|       21.19| 2022-11-26 23:45:07|  2022-11-26 23:59:19|\n|            23|          7| 0.2286111111111111|            1.0|         4.35|       1.0|       22.25| 2022-11-26 23:01:20|  2022-11-26 23:15:03|\n|            23|          7|0.09638888888888889|            1.0|         1.51|       1.0|        11.3| 2022-11-26 23:22:31|  2022-11-26 23:28:18|\n|            23|          7|0.25083333333333335|            1.0|         3.49|       1.0|        19.8| 2022-11-26 23:30:07|  2022-11-26 23:45:10|\n|            23|          7|0.22527777777777777|            1.0|         3.11|       1.0|       20.38| 2022-11-26 23:34:16|  2022-11-26 23:47:47|\n|            23|          7| 0.3713888888888889|            1.0|         4.01|       1.0|        24.0| 2022-11-26 23:58:35|  2022-11-27 00:20:52|\n|            23|          7| 0.5277777777777778|            1.0|         26.2|       1.0|       85.01| 2022-11-26 23:28:56|  2022-11-27 00:00:36|\n|            23|          7|0.29638888888888887|            2.0|         2.97|       1.0|        19.8| 2022-11-26 23:13:00|  2022-11-26 23:30:47|\n|            23|          7|0.45416666666666666|            2.0|          6.4|       1.0|       31.56| 2022-11-26 23:44:42|  2022-11-27 00:11:57|\n|            23|          7|0.09888888888888889|            2.0|         1.73|       1.0|       12.96| 2022-11-26 23:09:53|  2022-11-26 23:15:49|\n|            23|          7|0.13027777777777777|            1.0|         1.74|       1.0|        12.3| 2022-11-26 23:18:37|  2022-11-26 23:26:26|\n|            23|          7|0.35388888888888886|            1.0|         5.19|       1.0|        22.8| 2022-11-26 23:43:30|  2022-11-27 00:04:44|\n|            23|          7|0.05444444444444444|            1.0|          1.4|       1.0|         9.8| 2022-11-26 23:06:20|  2022-11-26 23:09:36|\n+--------------+-----------+-------------------+---------------+-------------+----------+------------+--------------------+---------------------+\nonly showing top 20 rows\n\nNumber of rows in the filtered data: 37411142\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import hour, dayofweek, expr\n",
    "\n",
    "# Assuming you have already loaded the combined_data DataFrame\n",
    "combined_data.createOrReplaceTempView(\"taxi_data\")\n",
    "\n",
    "# Use Spark SQL to filter data for 2021 and 2022 and select the desired columns\n",
    "filtered_data = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        hour(tpep_pickup_datetime) AS Hour_of_Pickup,\n",
    "        dayofweek(tpep_pickup_datetime) AS Day_of_Week,\n",
    "        (unix_timestamp(tpep_dropoff_datetime) - unix_timestamp(tpep_pickup_datetime)) / 3600 AS trip_duration,  -- Convert seconds to hours for trip duration\n",
    "        Passenger_count,\n",
    "        trip_distance,\n",
    "        RateCodeID,\n",
    "        total_amount, \n",
    "        tpep_pickup_datetime, \n",
    "        tpep_dropoff_datetime\n",
    "    FROM\n",
    "        taxi_data\n",
    "    \n",
    "\"\"\")\n",
    "\n",
    "# Show a sample of the filtered data\n",
    "filtered_data.show()\n",
    "\n",
    "# Save the filtered data as a new Parquet file\n",
    "filtered_data.write.mode(\"overwrite\").parquet(\"/FilteredData/filtered_taxi_data.parquet\")\n",
    "\n",
    "# To check the number of rows in the filtered data\n",
    "filtered_data_count = filtered_data.count()\n",
    "print(\"Number of rows in the filtered data:\", filtered_data_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44644d3a-a75a-4a8e-a2ee-e05e3b63ef93",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data Splitting: Training and Test Sets\n",
    "\n",
    "In this section, we split our filtered and preprocessed data into two distinct sets: a training dataset and a test dataset. The purpose of this split is to reserve a portion of the data for evaluating the performance of our predictive models. Here's what we're doing:\n",
    "\n",
    "1. **Filtering Data for Training and Test:** We divide the data into two time periods:\n",
    "   - **Training Dataset (January to September):** We select taxi trips that occurred between January and September, which will be used for training our predictive models.\n",
    "   - **Test Dataset (October to December):** We choose taxi trips from October to December, which will serve as our test dataset for model evaluation.\n",
    "\n",
    "2. **Data Saving:** After splitting the data, we save both the training and test datasets as separate Parquet files. This allows us to load and use these datasets for training and evaluating machine learning models.\n",
    "\n",
    "3. **Data Count:** We also provide information about the number of rows in each dataset for reference.\n",
    "\n",
    "This splitting ensures that our model is trained on historical data and evaluated on more recent data, helping us assess its generalization performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba05b132-bec1-4b10-8fab-6236e7eaf7b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the training data: 27697639\nNumber of rows in the test data: 9713503\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import month\n",
    "\n",
    "# Filter data for training dataset (January to September)\n",
    "train_data = filtered_data.filter((month(filtered_data[\"tpep_pickup_datetime\"]) >= 1) & (month(filtered_data[\"tpep_pickup_datetime\"]) <= 9))\n",
    "\n",
    "# Filter data for test dataset (October to December)\n",
    "test_data = filtered_data.filter((month(filtered_data[\"tpep_pickup_datetime\"]) >= 10) & (month(filtered_data[\"tpep_pickup_datetime\"]) <= 12))\n",
    "\n",
    "# Save the filtered training and test data as Parquet files\n",
    "train_data.write.mode(\"overwrite\").parquet(\"/FilteredData/train_taxi_data.parquet\")\n",
    "test_data.write.mode(\"overwrite\").parquet(\"/FilteredData/test_taxi_data.parquet\")\n",
    "\n",
    "# To check the number of rows in each dataset\n",
    "train_data_count = train_data.count()\n",
    "test_data_count = test_data.count()\n",
    "print(\"Number of rows in the training data:\", train_data_count)\n",
    "print(\"Number of rows in the test data:\", test_data_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f4ec758-34f6-4329-ab9f-5caa71f7cb2e",
     "showTitle": false,
     "title": ""
    },
    "id": "2qXQdyNQvaTz"
   },
   "source": [
    "Copy the dataframe into a new variable called `df_features`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b414ae77-537d-4bd9-a977-33e0e2372ed2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "                            \n",
    "train_data= spark.read.parquet(\"/FilteredData/train_taxi_data.parquet\")\n",
    "test_data=spark.read.parquet(\"/FilteredData/test_taxi_data.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "291dcfc9-e97a-44ba-adac-6d718ef07341",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Hour_of_Pickup: integer (nullable = true)\n |-- Day_of_Week: integer (nullable = true)\n |-- trip_duration: double (nullable = true)\n |-- Passenger_count: double (nullable = true)\n |-- trip_distance: double (nullable = true)\n |-- RateCodeID: double (nullable = true)\n |-- total_amount: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "filtered_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e889a862-1987-4b0e-a34b-b842df4b662d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In the schema, it seems that we have correctly chosen integer for categorical features (Hour_of_Pickup, Day_of_Week, Passenger_count, RateCodeID) and double for continuous features (trip_duration, trip_distance, total_amount), which is a reasonable choice based on the nature of these features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b350c53-fdca-4ecc-9c4a-6e3dac19e33c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Feature Correlation Analysis\n",
    "\n",
    "In this section, we perform a correlation analysis to understand the relationships between our selected features and the target variable, `Total_amount`. Here's what we're doing:\n",
    "\n",
    "1. **Selecting Features:** We have chosen a set of features, including `Hour_of_Pickup`, `Day_of_Week`, `trip_duration`, `Passenger_count`, `trip_distance`, and `RateCodeID`, to analyze their correlation with `Total_amount`. These features were intuitively selected based on our understanding of taxi fare determinants.\n",
    "\n",
    "2. **Calculating Correlation Coefficients:** For each selected feature, we calculate the correlation coefficient with `Total_amount`. The correlation coefficient measures the strength and direction of the linear relationship between two variables.\n",
    "\n",
    "3. **Storing Results:** We store the computed correlation coefficients in a dictionary called `correlation_coefficients`.\n",
    "\n",
    "4. **Printing Results:** Finally, we print the correlation coefficients to understand how each feature is related to the total fare amount. Positive values indicate a positive correlation, while negative values suggest a negative correlation. A value closer to 1 or -1 indicates a stronger relationship, while values close to 0 suggest a weaker relationship.\n",
    "\n",
    "Our initial selection of these features was guided by common intuitions about taxi fare determinants:\n",
    "\n",
    "- `Hour_of_Pickup`: We anticipated that the time of day when a taxi trip starts could influence fares, with potential differences during rush hours or late at night.\n",
    "\n",
    "- `Day_of_Week`: We considered that fares might vary based on the day of the week, especially on weekends or holidays.\n",
    "\n",
    "- `trip_duration`: Longer trips often incur higher fares, so we included this as a potential predictor.\n",
    "\n",
    "- `Passenger_count`: Different taxi services may have varying pricing structures based on passenger count.\n",
    "\n",
    "- `trip_distance`: The distance traveled is a fundamental factor in fare calculation, and we expected a strong correlation.\n",
    "\n",
    "- `RateCodeID`: We included this as it represents different fare structures, such as standard rates, airport fares, etc.\n",
    "\n",
    "This analysis helps us identify which features may have a significant impact on predicting taxi fare amounts and guides feature selection for our machine learning models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b12fd8d-e9c8-4882-a8ad-70d7914b28c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between Hour_of_Pickup and Total_amount: DenseMatrix([[1.00000000e+00, 1.01103732e-04],\n             [1.01103732e-04, 1.00000000e+00]])\nCorrelation between Day_of_Week and Total_amount: DenseMatrix([[ 1.       , -0.0029578],\n             [-0.0029578,  1.       ]])\nCorrelation between trip_duration and Total_amount: DenseMatrix([[1.        , 0.03141544],\n             [0.03141544, 1.        ]])\nCorrelation between Passenger_count and Total_amount: DenseMatrix([[1.        , 0.00520525],\n             [0.00520525, 1.        ]])\nCorrelation between trip_distance and Total_amount: DenseMatrix([[1.        , 0.13896552],\n             [0.13896552, 1.        ]])\nCorrelation between RateCodeID and Total_amount: DenseMatrix([[1.        , 0.09122573],\n             [0.09122573, 1.        ]])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# List of features to calculate correlation with Total_amount\n",
    "features = [\n",
    "    \"Hour_of_Pickup\",\n",
    "    \"Day_of_Week\",\n",
    "    \"trip_duration\",\n",
    "    \"Passenger_count\",\n",
    "    \"trip_distance\",\n",
    "    \"RateCodeID\"\n",
    "]\n",
    "\n",
    "# Create an empty dictionary to store the correlation coefficients\n",
    "correlation_coefficients = {}\n",
    "\n",
    "# Iterate through the features and calculate correlation with Total_amount\n",
    "for feature in features:\n",
    "    # Assemble the feature and Total_amount into a vector\n",
    "    vector_assembler = VectorAssembler(inputCols=[feature, \"total_amount\"], outputCol=\"features\")\n",
    "    assembled_data = vector_assembler.transform(train_data)\n",
    "\n",
    "    # Calculate the correlation matrix\n",
    "    corr_matrix = Correlation.corr(assembled_data, \"features\")\n",
    "\n",
    "    # Extract the correlation coefficient between the feature and Total_amount\n",
    "    correlation_coefficient = corr_matrix.collect()[0][0]\n",
    "\n",
    "    # Store the correlation coefficient in the dictionary\n",
    "    correlation_coefficients[feature] = correlation_coefficient\n",
    "\n",
    "# Print the correlation coefficients\n",
    "for feature, coefficient in correlation_coefficients.items():\n",
    "    print(f\"Correlation between {feature} and Total_amount: {coefficient}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7967c01c-5e79-49a9-b009-e58df1d97d33",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Interpreting Correlation Results:**\n",
    "\n",
    "1. The correlation coefficient between `Hour_of_Pickup` and `Total_amount` is approximately 0.0001, indicating an extremely weak positive correlation. This suggests that the time of day when a trip starts has minimal impact on the total fare amount.\n",
    "\n",
    "2. For `Day_of_Week` and `Total_amount`, the correlation coefficient is close to -0.003, also indicating a very weak correlation. Day of the week seems to have a negligible influence on taxi fares.\n",
    "\n",
    "3. `trip_duration` exhibits a slightly stronger positive correlation of approximately 0.0314 with `Total_amount`. Longer trip durations are associated with higher fares, but the correlation is still relatively weak.\n",
    "\n",
    "4. `Passenger_count` has a correlation coefficient of about 0.0052 with `Total_amount`, indicating a very weak positive correlation. The number of passengers in a taxi appears to have a minimal impact on fare.\n",
    "\n",
    "5. The `trip_distance` feature shows a moderate positive correlation with `Total_amount`, with a coefficient of approximately 0.139. This suggests that fare amounts tend to increase as the distance traveled becomes greater.\n",
    "\n",
    "6. `RateCodeID` demonstrates a somewhat stronger positive correlation with `Total_amount`, around 0.0912. Different rate codes, which represent fare structures, can influence taxi fares to some extent.\n",
    "\n",
    "While some features exhibit weak correlations with the total fare amount, it's essential to note that correlation does not imply causation. Further analysis and machine learning modeling will help us better understand the predictive power of these features in estimating taxi fares.\n",
    "**Next Steps:**\n",
    "With the said, it's still prudent to check if the performance of the model improves with the combination of these features, that might give us some information about the data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf83e74e-59c7-4564-abff-9786f9a6393a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "To gain further insights into the data and assess the potential impact of feature combinations, we will embark on a multi-step modeling journey. Our objective is to determine if incorporating these selected features enhances predictive accuracy. Here's our roadmap:\n",
    "## Predictive Modeling Approach\n",
    "\n",
    "In our predictive modeling process, we follow a step-by-step approach to build and assess the performance of different models for predicting total fare amounts. The primary goal is to enhance the accuracy of fare predictions and gain insights into the underlying data patterns. The key steps in our modeling approach are as follows:\n",
    "\n",
    "1. **Baseline Model:** We start with a baseline model, which is a fundamental approach to set a benchmark for our predictions. In this step, we calculate the average paid amount per trip in the training dataset. This straightforward baseline model provides a baseline metric for comparison.\n",
    "\n",
    "2. **Linear Regression:** After establishing the baseline, we employ Linear Regression, a widely-used regression technique, to build our predictive model. Linear Regression helps us explore the linear relationships between input features and total fare amounts. We assess whether this linear model can outperform our baseline model and provide more accurate predictions.\n",
    "\n",
    "3. **Random Forest Regression:** Following the LR assessment, we leverage the Random Forest Regressor, a robust machine learning algorithm, to build a predictive model. By doing so, we aim to investigate whether this ensemble method can outperform our baseline.\n",
    "\n",
    "These sequential steps help us gauge the effectiveness of our feature combination strategy and provide valuable insights into how well our models capture the underlying patterns in the data. Ultimately, this process aids in making informed decisions about feature selection and model selection for more accurate predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1164a90e-dd1b-4e69-bcc3-9cfb4f44a76d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n|Passenger_count|\n+---------------+\n|            7.0|\n|            1.0|\n|            4.0|\n|            3.0|\n|            2.0|\n|            6.0|\n|            5.0|\n+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have already loaded the filtered_data DataFrame\n",
    "unique_passenger_counts = filtered_data.select(\"Passenger_count\").distinct()\n",
    "\n",
    "# Show the unique passenger counts\n",
    "unique_passenger_counts.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "544e9007-f179-48d1-bb68-9d62d4d5954d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa9fd06a-8f2f-4f79-9308-a35ba0e9d6bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Average Amount Model: 111.16547072874559\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Calculate the average paid amount per trip from Part 2 Q3\n",
    "avg_amount = 43.47990516867878\n",
    "\n",
    "# Add a new column with the average amount as the prediction\n",
    "baseline_predictions = test_data.withColumn(\"prediction\", F.lit(avg_amount))\n",
    "\n",
    "# Evaluate the performance of the baseline model\n",
    "evaluator = RegressionEvaluator(labelCol=\"total_amount\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "baseline_rmse = evaluator.evaluate(baseline_predictions)\n",
    "print(\"RMSE for Average Amount Model:\", baseline_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42fb297f-caa4-4a11-8553-a057f3bdaeaa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6635bfae-8a5e-4e7f-aed0-dfa47aee5ad9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 7.736532279687777\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Define your features and create a vector assembler\n",
    "features = [\n",
    "    \"Hour_of_Pickup\",\n",
    "    \"Day_of_Week\",\n",
    "    \"trip_duration\",\n",
    "    \"Passenger_count\",\n",
    "    \"trip_distance\",\n",
    "    \"RateCodeID\"\n",
    "]\n",
    "\n",
    "vector_assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "\n",
    "# Define the Linear Regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"total_amount\")\n",
    "\n",
    "# Create a pipeline with the vector assembler and Linear Regression\n",
    "pipeline = Pipeline(stages=[vector_assembler, lr])\n",
    "\n",
    "# Fit the model on the training data\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate the model using a regression evaluator\n",
    "evaluator = RegressionEvaluator(labelCol=\"total_amount\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "# Print the RMSE\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2083df86-7ceb-42f2-995f-35b34aec50a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on Train Data: 107.7431383362781\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = model.transform(train_data)\n",
    "\n",
    "# Evaluate the model using a regression evaluator\n",
    "evaluator = RegressionEvaluator(labelCol=\"total_amount\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "# Print the RMSE\n",
    "print(\"Root Mean Squared Error (RMSE) on Train Data:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1543009a-3d6c-46da-955d-4fbce3b3ba6d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0df8208-9146-4de8-b6f4-957e31a8d122",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline  \n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "# Define the features and create a vector assembler\n",
    "features = [\n",
    "    \"Hour_of_Pickup\",\n",
    "    \"Day_of_Week\",\n",
    "    \"trip_duration\",\n",
    "    \"Passenger_count\",\n",
    "    \"trip_distance\",\n",
    "    \"RateCodeID\"\n",
    "]\n",
    "vector_assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "\n",
    "# Define the Random Forest Regressor\n",
    "rf = RandomForestRegressor(labelCol=\"total_amount\", seed=42)\n",
    "\n",
    "# Create a pipeline with the defined stages\n",
    "pipeline = Pipeline(stages=[vector_assembler, rf])\n",
    "\n",
    "# Fit the model on the training data\n",
    "model = pipeline.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61f3cb4b-2633-49c0-87e0-9b851eba4271",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Specify the path to save the model\n",
    "model_path = \"/Machine_Learning/model\"\n",
    "\n",
    "# Save the model to the specified path\n",
    "model.save(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adef4c62-1be6-4697-87e7-ce51056068c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 107.74179680830962\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = model.transform(train_data)\n",
    "\n",
    "# Evaluate the model using a regression evaluator and calculate RMSE\n",
    "evaluator = RegressionEvaluator(labelCol=\"total_amount\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on Train Data:\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68dbe6ec-6123-491a-a587-976de1ef2c81",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 7.64042504062487\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate the model using a regression evaluator and calculate RMSE\n",
    "evaluator = RegressionEvaluator(labelCol=\"total_amount\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on Test Data:\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d345789-a721-450e-b3ea-867cb1edb46d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 2.732453388553179\nR-squared (R2): 0.019652836091647807\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Evaluate the model using Mean Absolute Error (MAE)\n",
    "evaluator_mae = RegressionEvaluator(labelCol=\"total_amount\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "mae = evaluator_mae.evaluate(predictions)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "\n",
    "# Evaluate the model using R-squared (R2)\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"total_amount\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2 = evaluator_r2.evaluate(predictions)\n",
    "print(\"R-squared (R2):\", r2)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Predictive Modelling- Part-4",
   "widgets": {}
  },
  "colab": {
   "collapsed_sections": [
    "c5sggbHSvaT8",
    "EMl-L1ZAvaUE"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
